{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "n_state = env.observation_space\n",
    "print('State matrix:', n_state, 'number of state', n_state)\n",
    "\n",
    "n_action = env.action_space\n",
    "print('number of action:', n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, Q, epsilon, n_action):\n",
    "    \"\"\"\n",
    "    Run a episode and performs epsilon-greedy policy\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param Q: Q-function\n",
    "    @param epsilon: the trade-off between exploration and exploitation\n",
    "    @param n_action: action space\n",
    "    @return: resulting states, actions and rewards for the entire episode\n",
    "    \"\"\"\n",
    "    state, info = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with on-policy MC control with epsilon_greedy\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @param epsilon: the trade-off between exploration and exploitation\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(\"Training episode {}\".format(episode+1))\n",
    "        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "        for state_action, return_t in G.items():\n",
    "            state, action = state_action\n",
    "\n",
    "            G_sum[state_action] += return_t\n",
    "            N[state_action] += 1\n",
    "            Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "\n",
    "n_episode = 100000\n",
    "epsilon = 0.1\n",
    "\n",
    "optimal_Q, optimal_policy = mc_control_epsilon_greedy(env, gamma, n_episode, epsilon)\n",
    "\n",
    "def simulate_episode(env, policy):\n",
    "    state, info = env.reset()\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        action = policy[state]\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 50000\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(\"Testing episode {}\".format(episode+1))\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))\n",
    "\n",
    "print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
