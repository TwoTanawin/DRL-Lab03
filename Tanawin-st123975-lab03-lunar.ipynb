{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00715313  1.4124196  -0.7245581   0.06662139  0.00829555  0.16412328\n",
      "  0.          0.        ]\n",
      "(0, 14, -7, 0, 0, 1, 0, 0)\n",
      "Number of actions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\two-intel\\anaconda3\\envs\\my_torch\\lib\\site-packages\\gymnasium\\envs\\registration.py:788: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='rgp_array_list' that is not in the possible render_modes (['human', 'rgb_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgp_array_list\")\n",
    "state, info = env.reset()\n",
    "\n",
    "print(state)\n",
    "\n",
    "state = tuple((state * 10).astype(int))\n",
    "print(state)\n",
    "\n",
    "n_action = env.action_space.n\n",
    "print('Number of actions:', n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, Q, epsilon, n_action):\n",
    "    \"\"\"\n",
    "    Run a episode and performs epsilon-greedy policy\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param Q: Q-function\n",
    "    @param epsilon: the trade-off between exploration and exploitation\n",
    "    @param n_action: action space\n",
    "    @return: resulting states, actions and rewards for the entire episode\n",
    "    \"\"\"\n",
    "    state, info = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        state_tuple = tuple((state * 10).astype(int))  # Convert state to a tuple\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        best_action = torch.argmax(Q[state_tuple]).item()  # Use state_tuple as key\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        actions.append(action)\n",
    "        states.append(state_tuple)  # Append state_tuple\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with on-policy MC control with epsilon_greedy\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @param epsilon: the trade-off between exploration and exploitation\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(\"Training episode {}\".format(episode+1))\n",
    "        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "        for state_action, return_t in G.items():\n",
    "            state, action = state_action\n",
    "\n",
    "            G_sum[state_action] += return_t\n",
    "            N[state_action] += 1\n",
    "            Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1000\n",
      "Training episode 2000\n",
      "Training episode 3000\n",
      "Training episode 4000\n",
      "Training episode 5000\n",
      "Training episode 6000\n",
      "Training episode 7000\n",
      "Training episode 8000\n",
      "Training episode 9000\n",
      "Training episode 10000\n",
      "Training episode 11000\n",
      "Training episode 12000\n",
      "Training episode 13000\n",
      "Training episode 14000\n",
      "Training episode 15000\n",
      "Training episode 16000\n",
      "Training episode 17000\n",
      "Training episode 18000\n",
      "Training episode 19000\n",
      "Training episode 20000\n",
      "Training episode 21000\n",
      "Training episode 22000\n",
      "Training episode 23000\n",
      "Training episode 24000\n",
      "Training episode 25000\n",
      "Training episode 26000\n",
      "Training episode 27000\n",
      "Training episode 28000\n",
      "Training episode 29000\n",
      "Training episode 30000\n",
      "Training episode 31000\n",
      "Training episode 32000\n",
      "Training episode 33000\n",
      "Training episode 34000\n",
      "Training episode 35000\n",
      "Training episode 36000\n",
      "Training episode 37000\n",
      "Training episode 38000\n",
      "Training episode 39000\n",
      "Training episode 40000\n",
      "Training episode 41000\n",
      "Training episode 42000\n",
      "Training episode 43000\n",
      "Training episode 44000\n",
      "Training episode 45000\n",
      "Training episode 46000\n",
      "Training episode 47000\n",
      "Training episode 48000\n",
      "Training episode 49000\n",
      "Training episode 50000\n",
      "Training episode 51000\n",
      "Training episode 52000\n",
      "Training episode 53000\n",
      "Training episode 54000\n",
      "Training episode 55000\n",
      "Training episode 56000\n",
      "Training episode 57000\n",
      "Training episode 58000\n",
      "Training episode 59000\n",
      "Training episode 60000\n",
      "Training episode 61000\n",
      "Training episode 62000\n",
      "Training episode 63000\n",
      "Training episode 64000\n",
      "Training episode 65000\n",
      "Training episode 66000\n",
      "Training episode 67000\n",
      "Training episode 68000\n",
      "Training episode 69000\n",
      "Training episode 70000\n",
      "Training episode 71000\n",
      "Training episode 72000\n",
      "Training episode 73000\n",
      "Training episode 74000\n",
      "Training episode 75000\n",
      "Training episode 76000\n",
      "Training episode 77000\n",
      "Training episode 78000\n",
      "Training episode 79000\n",
      "Training episode 80000\n",
      "Training episode 81000\n",
      "Training episode 82000\n",
      "Training episode 83000\n",
      "Training episode 84000\n",
      "Training episode 85000\n",
      "Training episode 86000\n",
      "Training episode 87000\n",
      "Training episode 88000\n",
      "Training episode 89000\n",
      "Training episode 90000\n",
      "Training episode 91000\n",
      "Training episode 92000\n",
      "Training episode 93000\n",
      "Training episode 94000\n",
      "Training episode 95000\n",
      "Training episode 96000\n",
      "Training episode 97000\n",
      "Training episode 98000\n",
      "Training episode 99000\n",
      "Training episode 100000\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "n_episode = 100000\n",
    "epsilon = 0.1\n",
    "\n",
    "optimal_Q, optimal_policy = mc_control_epsilon_greedy(env, gamma, n_episode, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(env, policy):\n",
    "    state, info = env.reset()\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        state_tuple = tuple((state * 10).astype(int))  # Convert state to a tuple\n",
    "        action = policy[state_tuple]  # Use state_tuple as key\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing episode 1000\n",
      "Testing episode 2000\n",
      "Testing episode 3000\n",
      "Testing episode 4000\n",
      "Testing episode 5000\n",
      "Testing episode 6000\n",
      "Testing episode 7000\n",
      "Testing episode 8000\n",
      "Testing episode 9000\n",
      "Testing episode 10000\n",
      "Testing episode 11000\n",
      "Testing episode 12000\n",
      "Testing episode 13000\n",
      "Testing episode 14000\n",
      "Testing episode 15000\n",
      "Testing episode 16000\n",
      "Testing episode 17000\n",
      "Testing episode 18000\n",
      "Testing episode 19000\n",
      "Testing episode 20000\n",
      "Testing episode 21000\n",
      "Testing episode 22000\n",
      "Testing episode 23000\n",
      "Testing episode 24000\n",
      "Testing episode 25000\n",
      "Testing episode 26000\n",
      "Testing episode 27000\n",
      "Testing episode 28000\n",
      "Testing episode 29000\n",
      "Testing episode 30000\n",
      "Testing episode 31000\n",
      "Testing episode 32000\n",
      "Testing episode 33000\n",
      "Testing episode 34000\n",
      "Testing episode 35000\n",
      "Testing episode 36000\n",
      "Testing episode 37000\n",
      "Testing episode 38000\n",
      "Testing episode 39000\n",
      "Testing episode 40000\n",
      "Testing episode 41000\n",
      "Testing episode 42000\n",
      "Testing episode 43000\n",
      "Testing episode 44000\n",
      "Testing episode 45000\n",
      "Testing episode 46000\n",
      "Testing episode 47000\n",
      "Testing episode 48000\n",
      "Testing episode 49000\n",
      "Testing episode 50000\n"
     ]
    }
   ],
   "source": [
    "n_episode = 50000\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(\"Testing episode {}\".format(episode+1))\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning probability under the optimal policy: 0.0\n",
      "Losing probability under the optimal policy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))\n",
    "\n",
    "print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
