{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State matrix: Discrete(16) number of state Discrete(16)\n",
      "number of action: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"ansi_list\")\n",
    "\n",
    "n_state = env.observation_space\n",
    "print('State matrix:', n_state, 'number of state', n_state)\n",
    "\n",
    "n_action = env.action_space\n",
    "print('number of action:', n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy):\n",
    "    state, info = env.reset()\n",
    "    # this is the difference between MC and DP, find rewards and states\n",
    "    rewards = []\n",
    "    states = [state]\n",
    "    is_done = False\n",
    "    truncated = False\n",
    "    while not (is_done or truncated):\n",
    "        action = int(policy[state].item())\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        # keep all states and reward\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "    env.close()\n",
    "    # convert to torch\n",
    "    states = torch.tensor(states)\n",
    "    rewards = torch.tensor(rewards)\n",
    "\n",
    "    return states, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_first_visit(env, policy, gamma, n_episode):\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    N = torch.zeros(n_state)\n",
    "    for episode in range(n_episode):\n",
    "        # run 1 episode until end of the episode\n",
    "        states_t, rewards_t = run_episode(env, policy)\n",
    "        # print(states_t, rewards_t)\n",
    "        return_t = 0\n",
    "        first_visit = torch.zeros(n_state)\n",
    "        G = torch.zeros(n_state)\n",
    "        # take a look at the state and the reward from the last to first start\n",
    "        # calculate given policy\n",
    "        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):\n",
    "            # calculate rewards\n",
    "            # because the reward at the last can be only 0 or 1, otherwise are 0\n",
    "            # so the reward at first start will be smallest\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            # put the reward for the state into given policy\n",
    "            # as you can see, if we come in the same state, it will be replaced to the early time when visit\n",
    "            # That's why we call first-visit\n",
    "            G[state_t] = return_t\n",
    "            first_visit[state_t] = 1\n",
    "        # at the end of given policy calculation\n",
    "        # we need to update the state transition by summation them (prepare to average)\n",
    "        for state in range(n_state):\n",
    "            if first_visit[state] > 0:\n",
    "                V[state] += G[state]\n",
    "                N[state] += 1\n",
    "    # average state transition here\n",
    "    for state in range(n_state):\n",
    "        if N[state] > 0:\n",
    "            V[state] = V[state] / N[state]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function calculated by first-visit MC prediction:\n",
      " tensor([0.7252, 0.5179, 0.4311, 0.0000, 0.7252, 0.0000, 0.3547, 0.0000, 0.7252,\n",
      "        0.7261, 0.6491, 0.0000, 0.0000, 0.7964, 0.8873, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "n_episode = 10000\n",
    "\n",
    "optimal_policy = torch.tensor([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n",
    "value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)\n",
    "print('The value function calculated by first-visit MC prediction:\\n', value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
